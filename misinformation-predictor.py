# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p1xB44rr2TGmzXg9S_MVVRJvy0pBfDmF
"""

!pip install transformers sentence-transformers newspaper3k trafilatura torch torchvision

!pip install pytesseract
!apt install tesseract-ocr
!pip install sentence-transformers
!pip install Pillow

from PIL import Image
import pytesseract
from sentence_transformers import SentenceTransformer, util
from newspaper import Article  # or trafilatura
# Add your own summarize_text function using HuggingFace or LLM

# Load Sentence-BERT model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Extract text from image
def extract_text_from_image(image_path):
    image = Image.open(image_path)
    text = pytesseract.image_to_string(image)
    return text.strip()

# Compute cosine similarity
def compute_similarity(text1, text2):
    embedding1 = model.encode(text1, convert_to_tensor=True)
    embedding2 = model.encode(text2, convert_to_tensor=True)
    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()
    return similarity

# Headline and article content similarity
def check_headline_content_similarity(url):
    article = Article(url)
    article.download()
    article.parse()
    headline = article.title
    content = article.text
    similarity = compute_similarity(headline, content)
    return similarity, content

# Main pipeline
def is_clickbait(image_path, description, url, threshold=0.5):
    image_text = extract_text_from_image(image_path)
    sim1 = compute_similarity(image_text, description)

    sim2, content = check_headline_content_similarity(url)

    avg_similarity = (sim1 + sim2) / 2
    print(f"\nğŸ–¼ï¸ Image Text: {image_text}")
    print(f"ğŸ“ Description: {description}")
    print(f"ğŸ”— Headline â†” Content Similarity: {sim2:.2f}")
    print(f"ğŸ¯ Average Similarity: {avg_similarity:.2f}")

    if avg_similarity < threshold:
        print("âš ï¸ Clickbait Detected")
        summary = summarize_text(content)  # Define this separately
        return True, summary
    else:
        print("âœ… Not Clickbait")
        is_factual, context = retrieve_context_and_verify(claim_or_content, db)

    if is_factual:
        print("âœ… Claim is likely factual.")
    else:
        print("âš ï¸ Claim appears misleading or false.")
        print("ğŸ“Œ Supporting context (summary):")
        print(summarize_text(context))



from transformers import BartForConditionalGeneration, BartTokenizer
import torch

# Load the BART model and tokenizer
model_name = 'facebook/bart-large-cnn'
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

def summarize_text(text, max_input_len=1024, max_summary_len=150):
    # Tokenize input
    inputs = tokenizer.encode(text, return_tensors='pt', max_length=max_input_len, truncation=True)

    # Generate summary
    summary_ids = model.generate(
        inputs,
        max_length=max_summary_len,
        min_length=30,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )

    # Decode and return
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    return summary

pip install langchain openai faiss-cpu chromadb unstructured newspaper3k trafilatura

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import os

# Load and split your reference text corpus (assume you have news data in .txt files)
loader = TextLoader("fact_dataset.txt")  # Your reliable source content
docs = loader.load()

# Split into chunks
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(docs)

# Initialize embeddings
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Build FAISS index
db = FAISS.from_documents(split_docs, embedding_model)

def retrieve_context_and_verify(claim, db, threshold=0.6):
    # Search for relevant facts
    docs = db.similarity_search(claim, k=3)
    context = "\n".join([doc.page_content for doc in docs])

    print("\nğŸ” Retrieved Context:")
    print(context)

    # Now compare similarity
    from sentence_transformers import SentenceTransformer, util
    model = SentenceTransformer("all-MiniLM-L6-v2")

    claim_embedding = model.encode(claim, convert_to_tensor=True)
    context_embedding = model.encode(context, convert_to_tensor=True)

    similarity = util.pytorch_cos_sim(claim_embedding, context_embedding).item()
    print(f"\nğŸ¤– Similarity to context: {similarity:.2f}")

    is_factual = similarity > threshold
    return is_factual, context